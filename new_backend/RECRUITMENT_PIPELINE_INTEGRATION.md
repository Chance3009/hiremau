# HireMau Recruitment Pipeline Integration Guide

## âœ… **What's Been Completed**

### 1. **Database Schema (Simplified)**
- âœ… Removed AI fields that frontend doesn't use
- âœ… Added essential fields that match frontend forms
- âœ… Created proper relationships (event_jobs, job_applications)
- âœ… Fixed constraint violations

### 2. **Frontend Form Enhancement**
- âœ… Added essential candidate fields:
  - Current position, years experience
  - Education, experience summary
  - Availability, preferred work type
  - Salary expectations, LinkedIn/GitHub URLs
- âœ… Form now collects real data instead of hardcoded values

### 3. **Simplified Agent Processing**
- âœ… Created `add_candidate_simplified.py`
- âœ… Processes PDF and image documents
- âœ… Stores analysis in `company_table` (existing embedding infrastructure)
- âœ… Runs on port 5001

### 4. **Simplified Backend Service**
- âœ… Created `candidate_service_simplified.py`
- âœ… Integrates frontend â†’ database â†’ agent workflow
- âœ… Handles job applications and candidate processing

## ðŸ”„ **Complete Integration Flow**

```mermaid
graph LR
    A[Frontend Form] --> B[Backend API]
    B --> C[Database]
    B --> D[Agent Processing]
    D --> E[Document Analysis]
    E --> F[Embedding Storage]
    F --> G[Candidate Pipeline]
```

### **Flow Details:**
1. **Frontend** collects candidate data + files
2. **Backend** creates candidate record in database
3. **Agent** processes documents (resume, certificates)
4. **AI Analysis** extracts candidate information
5. **Storage** saves analysis as embeddings in company_table
6. **Pipeline** shows candidates in recruitment stages

## ðŸš€ **How to Run the Complete System**

### **Step 1: Apply Database Schema**
```sql
-- In Supabase SQL Editor, run:
-- Copy content from: new_backend/SIMPLIFIED_SCHEMA.sql
```

### **Step 2: Start the Simplified Agent**
```powershell
cd agent
python add_candidate_simplified.py
# Runs on http://localhost:5001
```

### **Step 3: Start the Backend (Updated)**
Update your main router to use the simplified service:

```python
# In new_backend/routers/candidates_router.py
from services.candidate_service_simplified import SimplifiedCandidateService

candidate_service = SimplifiedCandidateService()

@router.post("/candidates/create_with_processing")
async def create_candidate_with_processing(
    candidate_data: dict,
    files: List[dict] = None
):
    return await candidate_service.create_candidate_with_processing(
        candidate_data, files
    )
```

### **Step 4: Update Frontend Service**
Update `frontend/src/services/candidateService.ts` to call the new endpoint:

```typescript
export const createCandidateWithProcessing = async (formData: FormData) => {
  const response = await fetch('http://localhost:8001/candidates/create_with_processing', {
    method: 'POST',
    body: formData
  });
  return response.json();
};
```

## ðŸ“Š **Testing the Pipeline**

### **Test Case 1: Basic Candidate Creation**
1. Fill out the enhanced candidate form
2. Upload a resume PDF
3. Submit form
4. Check database: candidate record created
5. Check agent logs: document processed
6. Check company_table: analysis stored

### **Test Case 2: Job Application Flow**
1. Create candidate with event_id and job_id
2. Verify job_applications table has record
3. Update candidate stage to "screening"
4. Verify job_applications.status updated

### **Test Case 3: AI Analysis Retrieval**
1. Query company_table for candidate analysis
2. Filter by metadata->candidate_id
3. Extract AI insights for candidate matching

## ðŸ” **Database Verification**

```sql
-- Check candidates
SELECT id, name, email, stage, status, created_at FROM candidates ORDER BY created_at DESC LIMIT 10;

-- Check job applications
SELECT c.name, j.title, ja.status, ja.applied_at 
FROM job_applications ja
JOIN candidates c ON ja.candidate_id = c.id
JOIN jobs j ON ja.job_id = j.id
ORDER BY ja.applied_at DESC;

-- Check AI analysis (stored in company_table)
SELECT content, metadata 
FROM company_table 
WHERE metadata->>'document_type' = 'candidate_analysis' 
ORDER BY (metadata->>'processed_at')::timestamp DESC;
```

## ðŸŽ¯ **Key Benefits of Simplified Approach**

### **Frontend Benefits:**
- âœ… Form collects meaningful data
- âœ… Fields match database schema
- âœ… Malaysian context (MYR, local preferences)
- âœ… File upload with validation

### **Backend Benefits:**
- âœ… Single service handles complete workflow
- âœ… Proper error handling and logging
- âœ… Integrates with existing Supabase structure
- âœ… Reuses company_table for embeddings

### **Agent Benefits:**
- âœ… Simplified document processing
- âœ… Works with PDFs and images
- âœ… Stores analysis for retrieval
- âœ… No dependency on dropped tables

### **Pipeline Benefits:**
- âœ… End-to-end candidate tracking
- âœ… Job application workflow
- âœ… Stage progression management
- âœ… AI-powered candidate insights

## ðŸ”§ **Troubleshooting**

### **Agent Not Processing:**
- Check agent is running on port 5001
- Verify GOOGLE_API_KEY in environment
- Check file URLs are accessible

### **Database Errors:**
- Verify SIMPLIFIED_SCHEMA.sql was executed
- Check Supabase connection credentials
- Validate candidate data format

### **Frontend Issues:**
- Check form validation
- Verify API endpoint URLs
- Test file upload limits

## ðŸ“ˆ **Next Steps**

1. **Test Integration**: Run through complete pipeline
2. **Add Validation**: Enhance error handling
3. **UI Polish**: Improve candidate display
4. **Performance**: Optimize database queries
5. **Monitoring**: Add logging and metrics

This simplified approach gets you a **working recruitment pipeline** that:
- Collects real candidate data
- Processes documents with AI
- Stores everything properly
- Shows candidates in the pipeline
- Works within your 3-hour timeline!

## ðŸŽ‰ **Ready to Test!**

Run the system and test with a real candidate:
1. Open frontend candidate form
2. Fill in candidate details
3. Upload a resume
4. Submit and watch the magic happen!

The complete flow should work: **Frontend â†’ Database â†’ Agent â†’ Analysis â†’ Pipeline Display** 